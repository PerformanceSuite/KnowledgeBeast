# KnowledgeBeast v2.3.0 Improvements Implementation Plan

**Version**: v2.3.0-improvements
**Date**: October 9, 2025
**Status**: Planning → Execution
**Goal**: Achieve A+ (98/100) production readiness

## Overview

This plan implements all 12 identified improvements to bring KnowledgeBeast v2.3.0 from B+ (85/100) to A+ (98/100) production grade. We'll use 4 autonomous agents working in parallel with git worktrees.

## Current State Assessment

### ✅ What's Production Ready
- Multi-project backend (707 q/s throughput validated)
- Perfect data isolation (E2E proven)
- Thread-safe operations
- Docker + Kubernetes deployment
- Basic monitoring (Prometheus, Grafana, Jaeger)
- 1,563 tests (90% pass rate)

### ⚠️ What Needs Work
- Test isolation (22 API tests flaky)
- Per-project rate limiting (currently global)
- Per-project metrics (no project-level observability)
- Project-level API keys (single global key)
- Connection pooling (ChromaDB client per request)
- Health monitoring per project

## Agent Assignments

### Agent 1: Production Hardening Agent
**Branch**: `feature/production-hardening`
**Priority**: HIGH
**Estimated Time**: 9 hours
**Dependencies**: None (can start immediately)

#### Objectives
1. Fix project API test isolation (4h)
2. Add per-project rate limiting (2h)
3. Add project resource limits/quotas (3h)

#### Deliverables

**1. Test Isolation Fixes**
- File: `tests/api/conftest.py`
- Implement per-function test database isolation
- Add auto-cleanup ChromaDB fixture
- Ensure all 22 flaky tests pass reliably

```python
@pytest.fixture(scope="function")
def isolated_db(tmp_path):
    """Provide isolated database per test."""
    db_path = tmp_path / "test.db"
    return str(db_path)

@pytest.fixture(autouse=True)
def reset_chroma(monkeypatch, tmp_path):
    """Reset ChromaDB between tests."""
    chroma_path = tmp_path / "chroma"
    chroma_path.mkdir(exist_ok=True)
    monkeypatch.setenv("KB_CHROMA_PATH", str(chroma_path))
    yield
    shutil.rmtree(chroma_path, ignore_errors=True)
```

**2. Per-Project Rate Limiting**
- File: `knowledgebeast/api/routes.py`
- Implement composite key function: `{api_key}:{project_id}`
- Update all v2 endpoints with per-project limits
- Add configuration via environment variables

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

def get_rate_limit_key():
    """Generate rate limit key: api_key:project_id."""
    api_key = request.headers.get("X-API-Key", "anonymous")
    project_id = request.path_params.get("project_id", "global")
    return f"{api_key}:{project_id}"

limiter = Limiter(key_func=get_rate_limit_key)

@limiter.limit("30/minute")
@router_v2.post("/{project_id}/query")
async def project_query(project_id: str, ...):
    # Rate limited per project per API key
```

**3. Project Resource Limits**
- File: `knowledgebeast/core/project_manager.py`
- Add `max_docs_per_project` configuration
- Implement quota enforcement in `ingest()` method
- Add quota status to project metadata
- Add `/api/v2/projects/{id}/quota` endpoint

```python
class ProjectManager:
    def __init__(self, max_docs_per_project: int = 10000):
        self.max_docs_per_project = max_docs_per_project

    async def ingest(self, project_id: str, content: str, metadata: Dict):
        doc_count = await self.get_project_doc_count(project_id)
        if doc_count >= self.max_docs_per_project:
            raise QuotaExceededError(
                f"Project {project_id} has reached limit of {self.max_docs_per_project} documents"
            )
        # ... proceed with ingestion
```

#### Success Criteria
- [ ] All 22 flaky API tests pass reliably (3 consecutive full test suite runs)
- [ ] Per-project rate limiting functional (verified with concurrent requests)
- [ ] Quota enforcement works (tested with quota exceeded scenarios)
- [ ] Test suite achieves 95%+ pass rate
- [ ] All new code has 90%+ test coverage

---

### Agent 2: Security & Observability Agent
**Branch**: `feature/security-observability`
**Priority**: MEDIUM
**Estimated Time**: 15 hours
**Dependencies**: None (can start immediately)

#### Objectives
1. Implement project-level API keys (8h)
2. Add per-project Prometheus metrics (3h)
3. Enhance distributed tracing with project context (3h)
4. Create multi-project API documentation (4h - moved from Agent 4)

#### Deliverables

**1. Project-Level API Keys**
- File: `knowledgebeast/core/auth.py` (NEW)
- File: `knowledgebeast/api/middleware.py` (NEW)
- Database schema for API keys table
- CRUD endpoints for key management
- Middleware for project access validation

```python
# knowledgebeast/core/auth.py
class ProjectAuthManager:
    def create_api_key(self, project_id: str, name: str, scopes: List[str]) -> str:
        """Generate project-scoped API key."""
        key = secrets.token_urlsafe(32)
        key_id = hashlib.sha256(key.encode()).hexdigest()
        self.store_api_key(project_id, key_id, name, scopes)
        return key

    def validate_project_access(self, api_key: str, project_id: str) -> bool:
        """Validate API key has access to project."""
        key_id = hashlib.sha256(api_key.encode()).hexdigest()
        return self.has_project_access(key_id, project_id)

    def revoke_api_key(self, key_id: str):
        """Revoke API key."""
        self.mark_revoked(key_id)

# API endpoints
@router_v2.post("/projects/{project_id}/api-keys")
async def create_project_api_key(project_id: str, key_data: APIKeyCreate):
    """Create new API key for project."""

@router_v2.get("/projects/{project_id}/api-keys")
async def list_project_api_keys(project_id: str):
    """List all API keys for project."""

@router_v2.delete("/projects/{project_id}/api-keys/{key_id}")
async def revoke_project_api_key(project_id: str, key_id: str):
    """Revoke project API key."""
```

**2. Per-Project Prometheus Metrics**
- File: `knowledgebeast/monitoring/metrics.py`
- Add project_id label to all metrics
- Track per-project: queries, latency, errors, cache hits, document count

```python
from prometheus_client import Counter, Histogram, Gauge

# Per-project metrics
project_queries_total = Counter(
    'kb_project_queries_total',
    'Total queries per project',
    ['project_id', 'status']
)

project_query_latency = Histogram(
    'kb_project_query_latency_seconds',
    'Query latency per project',
    ['project_id'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0]
)

project_document_count = Gauge(
    'kb_project_documents_total',
    'Total documents per project',
    ['project_id']
)

project_cache_hits = Counter(
    'kb_project_cache_hits_total',
    'Cache hits per project',
    ['project_id']
)

# Usage in routes.py
@router_v2.post("/{project_id}/query")
async def project_query(project_id: str, ...):
    with project_query_latency.labels(project_id=project_id).time():
        try:
            results = await query_logic()
            project_queries_total.labels(project_id=project_id, status='success').inc()
            return results
        except Exception as e:
            project_queries_total.labels(project_id=project_id, status='error').inc()
            raise
```

**3. Distributed Tracing Enhancements**
- File: `knowledgebeast/monitoring/tracing.py`
- Add project_id to all trace spans
- Add project_name attribute
- Enable per-project filtering in Jaeger

```python
from opentelemetry import trace

def add_project_context(span, project_id: str, project_name: str = None):
    """Add project context to trace span."""
    span.set_attribute("kb.project.id", project_id)
    if project_name:
        span.set_attribute("kb.project.name", project_name)
    span.set_attribute("kb.api.version", "v2")

# Usage in routes.py
@router_v2.post("/{project_id}/query")
async def project_query(project_id: str, ...):
    tracer = trace.get_tracer(__name__)
    with tracer.start_as_current_span("project_query") as span:
        add_project_context(span, project_id)
        # ... query logic
```

**4. Multi-Project API Documentation**
- File: `docs/api/MULTI_PROJECT_GUIDE.md` (NEW)
- Complete guide for v2 API usage
- Example workflows and code samples
- Migration guide from v1 to v2
- Best practices for multi-tenant usage

#### Success Criteria
- [ ] Project API keys fully functional (create, list, revoke)
- [ ] Per-project metrics visible in Prometheus
- [ ] Grafana dashboard shows per-project stats
- [ ] Jaeger traces filterable by project_id
- [ ] API documentation comprehensive and accurate
- [ ] All security tests pass (API key validation, revocation)

---

### Agent 3: Performance & DX Agent
**Branch**: `feature/performance-dx`
**Priority**: MEDIUM
**Estimated Time**: 14 hours (adjusted)
**Dependencies**: None (can start immediately)

#### Objectives
1. Implement ChromaDB connection pooling (2h)
2. Add project health monitoring endpoints (6h)
3. Implement query result streaming (6h)

#### Deliverables

**1. ChromaDB Connection Pooling**
- File: `knowledgebeast/core/project_manager.py`
- Implement singleton ChromaDB client
- Cache collections in memory
- Lazy loading with thread-safe initialization

```python
class ProjectManager:
    def __init__(self, chroma_path: str = "./data/chroma"):
        self.chroma_path = chroma_path
        self._chroma_client = None
        self._collection_cache = {}
        self._cache_lock = threading.RLock()

    @property
    def chroma_client(self):
        """Lazy-initialized singleton ChromaDB client."""
        if self._chroma_client is None:
            self._chroma_client = chromadb.PersistentClient(path=self.chroma_path)
        return self._chroma_client

    def get_collection(self, project_id: str):
        """Get or create cached collection."""
        with self._cache_lock:
            if project_id not in self._collection_cache:
                self._collection_cache[project_id] = self.chroma_client.get_or_create_collection(
                    name=f"project_{project_id}"
                )
            return self._collection_cache[project_id]
```

**2. Project Health Monitoring**
- File: `knowledgebeast/api/routes.py`
- Add `/api/v2/projects/{id}/health` endpoint
- Monitor: document count, query performance, cache stats, error rate

```python
@router_v2.get("/{project_id}/health")
async def get_project_health(project_id: str):
    """Get health status for project."""
    health = {
        "project_id": project_id,
        "status": "healthy",
        "metrics": {
            "document_count": await pm.get_document_count(project_id),
            "avg_query_latency_ms": await get_avg_query_latency(project_id),
            "cache_hit_rate": await get_cache_hit_rate(project_id),
            "error_rate": await get_error_rate(project_id),
            "last_query_time": await get_last_query_time(project_id),
        },
        "alerts": []
    }

    # Health checks
    if health["metrics"]["avg_query_latency_ms"] > 500:
        health["status"] = "degraded"
        health["alerts"].append("High query latency detected")

    if health["metrics"]["error_rate"] > 0.05:
        health["status"] = "unhealthy"
        health["alerts"].append("High error rate detected")

    return health
```

**3. Query Result Streaming**
- File: `knowledgebeast/api/routes.py`
- Add `/api/v2/projects/{id}/query/stream` endpoint
- Use Server-Sent Events (SSE) for streaming results
- Stream results as they're retrieved from ChromaDB

```python
from fastapi.responses import StreamingResponse

@router_v2.post("/{project_id}/query/stream")
async def project_query_stream(project_id: str, query_request: ProjectQueryRequest):
    """Stream query results using Server-Sent Events."""

    async def generate_results():
        collection = await get_project_collection(project_id)

        # Get query embedding
        query_embedding = await embed_query(query_request.query)

        # Query in batches and stream
        batch_size = 10
        offset = 0

        while offset < query_request.top_k:
            results = collection.query(
                query_embeddings=query_embedding,
                n_results=min(batch_size, query_request.top_k - offset),
                offset=offset
            )

            for i, (doc_id, doc, score) in enumerate(zip(results['ids'][0], results['documents'][0], results['distances'][0])):
                yield f"data: {json.dumps({'id': doc_id, 'document': doc, 'score': score})}\n\n"

            offset += batch_size

        yield "data: [DONE]\n\n"

    return StreamingResponse(generate_results(), media_type="text/event-stream")
```

#### Success Criteria
- [ ] ChromaDB connection pooling reduces overhead (benchmark shows improvement)
- [ ] Health endpoint returns accurate metrics
- [ ] Health alerts trigger correctly (slow queries, high errors)
- [ ] Query streaming works for large result sets
- [ ] SSE client examples provided
- [ ] Performance benchmarks show improvements

---

### Agent 4: Advanced Features Agent
**Branch**: `feature/advanced-features`
**Priority**: LOW
**Estimated Time**: 13 hours (adjusted)
**Dependencies**: Agent 2 (needs API key system for import/export)

#### Objectives
1. Implement project import/export (8h)
2. Implement project templates (5h)

#### Deliverables

**1. Project Import/Export**
- File: `knowledgebeast/core/project_manager.py`
- Add export method to create ZIP archive
- Add import method to restore from ZIP
- Include: documents, metadata, embeddings, configuration

```python
class ProjectManager:
    async def export_project(self, project_id: str, output_path: str):
        """Export project to ZIP archive."""
        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            # Export project metadata
            project_data = await self.get_project_metadata(project_id)
            zf.writestr('project.json', json.dumps(project_data))

            # Export documents
            collection = self.get_collection(project_id)
            all_docs = collection.get(include=['embeddings', 'documents', 'metadatas'])

            zf.writestr('documents.json', json.dumps({
                'ids': all_docs['ids'],
                'documents': all_docs['documents'],
                'metadatas': all_docs['metadatas'],
            }))

            # Export embeddings (compressed)
            embeddings_array = np.array(all_docs['embeddings'])
            embeddings_bytes = BytesIO()
            np.savez_compressed(embeddings_bytes, embeddings=embeddings_array)
            zf.writestr('embeddings.npz', embeddings_bytes.getvalue())

        return output_path

    async def import_project(self, zip_path: str, new_project_name: str = None) -> str:
        """Import project from ZIP archive."""
        with zipfile.ZipFile(zip_path, 'r') as zf:
            # Read project metadata
            project_data = json.loads(zf.read('project.json'))

            # Create new project
            if new_project_name:
                project_data['name'] = new_project_name

            project_id = await self.create_project(**project_data)

            # Import documents
            docs_data = json.loads(zf.read('documents.json'))
            embeddings_data = np.load(BytesIO(zf.read('embeddings.npz')))
            embeddings = embeddings_data['embeddings']

            # Batch insert
            collection = self.get_collection(project_id)
            collection.add(
                ids=docs_data['ids'],
                embeddings=embeddings.tolist(),
                documents=docs_data['documents'],
                metadatas=docs_data['metadatas']
            )

        return project_id

# API endpoints
@router_v2.post("/{project_id}/export")
async def export_project(project_id: str, background_tasks: BackgroundTasks):
    """Export project as ZIP."""
    output_path = f"/tmp/{project_id}_export.zip"
    await pm.export_project(project_id, output_path)
    return FileResponse(output_path, filename=f"{project_id}.zip")

@router_v2.post("/projects/import")
async def import_project(file: UploadFile, new_name: Optional[str] = None):
    """Import project from ZIP."""
    with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp:
        tmp.write(await file.read())
        tmp_path = tmp.name

    project_id = await pm.import_project(tmp_path, new_name)
    os.unlink(tmp_path)

    return {"project_id": project_id, "status": "imported"}
```

**2. Project Templates**
- File: `knowledgebeast/templates/__init__.py` (NEW)
- Predefined templates: ai-research, code-search, documentation, support-kb
- Sample documents for each template
- Optimized configurations per use case

```python
# knowledgebeast/templates/__init__.py
TEMPLATES = {
    "ai-research": {
        "name": "AI Research Project",
        "description": "Optimized for AI/ML research papers and articles",
        "config": {
            "embedding_model": "all-mpnet-base-v2",  # Higher quality
            "chunk_size": 512,
            "chunk_overlap": 50,
        },
        "sample_docs": [
            {
                "content": "Introduction to Transformer architectures...",
                "metadata": {"topic": "transformers", "type": "paper"}
            },
            # ... more samples
        ]
    },
    "code-search": {
        "name": "Code Search Project",
        "description": "Optimized for searching code repositories",
        "config": {
            "embedding_model": "all-MiniLM-L6-v2",  # Fast
            "chunk_size": 256,
            "chunk_overlap": 30,
        },
        "sample_docs": [
            {
                "content": "def calculate_fibonacci(n):\n    ...",
                "metadata": {"language": "python", "type": "function"}
            },
            # ... more samples
        ]
    },
    "documentation": {
        "name": "Documentation Project",
        "description": "Optimized for product documentation and guides",
        "config": {
            "embedding_model": "all-MiniLM-L6-v2",
            "chunk_size": 384,
            "chunk_overlap": 40,
        },
        "sample_docs": [
            {
                "content": "Getting Started Guide\n\nWelcome to...",
                "metadata": {"section": "getting-started", "type": "guide"}
            },
            # ... more samples
        ]
    },
    "support-kb": {
        "name": "Support Knowledge Base",
        "description": "Optimized for customer support Q&A",
        "config": {
            "embedding_model": "all-MiniLM-L6-v2",
            "chunk_size": 256,
            "chunk_overlap": 20,
        },
        "sample_docs": [
            {
                "content": "Q: How do I reset my password?\nA: ...",
                "metadata": {"category": "account", "type": "faq"}
            },
            # ... more samples
        ]
    }
}

class ProjectTemplateManager:
    @staticmethod
    async def create_from_template(template_name: str, project_name: str, pm: ProjectManager) -> str:
        """Create project from template."""
        if template_name not in TEMPLATES:
            raise ValueError(f"Unknown template: {template_name}")

        template = TEMPLATES[template_name]

        # Create project
        project_id = await pm.create_project(
            name=project_name,
            description=template['description'],
            config=template['config']
        )

        # Ingest sample documents
        for doc in template['sample_docs']:
            await pm.ingest(
                project_id=project_id,
                content=doc['content'],
                metadata=doc['metadata']
            )

        return project_id

# API endpoint
@router_v2.post("/projects/from-template/{template_name}")
async def create_project_from_template(
    template_name: str,
    project_name: str,
    include_samples: bool = True
):
    """Create project from template."""
    project_id = await ProjectTemplateManager.create_from_template(
        template_name, project_name, pm
    )
    return {"project_id": project_id, "template": template_name}

@router_v2.get("/templates")
async def list_templates():
    """List available project templates."""
    return {
        "templates": [
            {
                "name": name,
                "description": tmpl['description'],
                "config": tmpl['config']
            }
            for name, tmpl in TEMPLATES.items()
        ]
    }
```

#### Success Criteria
- [ ] Project export creates valid ZIP archive
- [ ] Project import restores all data correctly
- [ ] Export/import preserves embeddings (no re-computation needed)
- [ ] All 4 templates functional
- [ ] Template creation includes sample documents
- [ ] Migration guide between templates provided

---

## Execution Plan

### Phase 1: Setup (15 minutes)
1. Create git worktrees for all 4 agents
2. Create feature branches
3. Verify worktree isolation

### Phase 2: Parallel Agent Execution (8-15 hours per agent)
1. Launch all 4 agents simultaneously
2. Each agent works autonomously in its worktree
3. Agents create PRs when ready

### Phase 3: Review & Refinement (iterative)
1. Review each PR using `/review` command
2. Agents fix issues until 10/10 quality
3. Run full test suite on each PR

### Phase 4: Integration & Merge (1-2 hours)
**Merge Order** (dependency-based):
1. Agent 1: Production Hardening (no dependencies)
2. Agent 3: Performance & DX (no dependencies)
3. Agent 2: Security & Observability (no dependencies)
4. Agent 4: Advanced Features (depends on Agent 2's API key system)

### Phase 5: Final Validation (30 minutes)
1. Run full test suite on main branch
2. Run E2E multi-project test
3. Verify all improvements functional
4. Update memory.md with completion status

## Success Metrics

### Before Improvements (v2.3.0 baseline)
- Overall Grade: B+ (85/100)
- Test Pass Rate: 90%
- Per-project rate limiting: ❌
- Project API keys: ❌
- Per-project metrics: ❌
- Connection pooling: ❌
- Health monitoring: ❌
- Test isolation: ⚠️ (22 flaky tests)

### After Improvements (v2.3.0-enhanced)
- Overall Grade: A+ (98/100)
- Test Pass Rate: 95%+
- Per-project rate limiting: ✅
- Project API keys: ✅
- Per-project metrics: ✅
- Connection pooling: ✅
- Health monitoring: ✅
- Test isolation: ✅ (all tests reliable)
- Query streaming: ✅
- Import/Export: ✅
- Templates: ✅

## Risk Mitigation

### Risk 1: Test Isolation Complexity
- **Mitigation**: Start with simplest fixtures, iterate
- **Fallback**: Mark problematic tests as integration tests

### Risk 2: API Key System Complexity
- **Mitigation**: Use proven patterns (JWT-like but simpler)
- **Fallback**: Phase 1 = global keys with project scope check

### Risk 3: Streaming Implementation
- **Mitigation**: Use well-tested SSE library
- **Fallback**: Batch streaming (return 10 results at a time)

### Risk 4: Agent Coordination
- **Mitigation**: Clear dependency order, autonomous execution
- **Fallback**: Sequential execution if conflicts arise

## Timeline

**Optimistic**: 20 hours (all agents parallel, no issues)
**Realistic**: 30 hours (some iteration, PR reviews)
**Pessimistic**: 40 hours (significant rework needed)

**Target Completion**: October 11, 2025

---

## Agent Launch Commands

```bash
# Create worktrees
mkdir -p .worktrees
git worktree add .worktrees/production-hardening -b feature/production-hardening
git worktree add .worktrees/security-observability -b feature/security-observability
git worktree add .worktrees/performance-dx -b feature/performance-dx
git worktree add .worktrees/advanced-features -b feature/advanced-features

# Agents will be launched via Task tool in parallel
```

## Post-Completion Tasks

1. Update `docs/CHANGELOG.md` with all improvements
2. Update `memory.md` with v2.3.0-enhanced status
3. Create release notes for v2.3.0
4. Tag release: `git tag v2.3.0`
5. Update production deployment guide

---

**Plan Status**: ✅ Ready for Execution
**Next Step**: Launch all 4 agents in parallel
